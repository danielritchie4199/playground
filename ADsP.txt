ADsP 김계철 교재 전체 내용 (k_001 폴더)
============================================================

이 파일은 k_001 폴더에 있는 ADsP 김계철 교재의 16페이지부터 72페이지까지의 
모든 내용을 순서대로 분석하여 요약하지 않고 전체 내용을 그대로 기록한 것입니다.

사용자가 제공하는 각 페이지별 이미지 내용을 완전히 기록합니다.

============================================================

페이지 16
============================================================

시험에 출제되는 핵심 내용 요약 정리

1과목 데이터 이해

001 데이터의 정의★

① 데이터의 존재적 특성
• 데이터는 객관적 사실로서 존재하는 모든 형태의 정보를 의미
• 개별 데이터는 단순히 객관적 사실을 나타내는 정보일 뿐, 의사결정이나 인사이트 도출에 부족함
예 온도 - "서울의 오늘 기온은 25도입니다."

② 데이터의 당위적 특성
• 데이터는 추론, 예측, 전망, 추정 등 의사결정을 위한 근거로 기능
예 온도 데이터 분석 : "서울의 평균 기온이 5년간 상승했으므로 기후 변화 대응이 필요하다."

002 데이터의 유형★★

• 정성적 데이터와 정량적 데이터는 데이터의 형태, 분석 방식, 결과 해석 등에서 차이가 있다.

① 정성적 데이터(Qualitative Data)
- 데이터의 형태와 구조가 정해져 있지 않음
- 정성적 데이터는 수치로 직접 표현할 수 없고, 맥락과 주관적 해석에 의존
- 텍스트, 음성, 이미지, 동영상 등의 다양한 형식을 포함
예 트위터, 페이스북에서 사용자의 게시글과 댓글

② 정량적 데이터(Quantitative Data)
- 정량적 데이터는 숫자, 기호, 또는 도형으로 표현되며, 정밀한 계산이 가능
- 계산과 분석 용이
- 데이터양이 증가하더라도 저장, 검색, 분석이 효율적
예 풍속(10m/s), 강수량(50mm)

003 암묵지 vs 형식지★

• 암묵지와 형식지는 지식의 표현 방식, 전달 방법, 저장 가능성 등 여러 기준에 따라 구분

① 암묵지(Tacit Knowledge)
• 학습과 체험을 통해 개인에게 습득된 지식으로, 겉으로 명확히 드러나지 않는 무형의 지식을 의미함
• 시행착오와 오랜 경험을 통해 체득된 지식으로, 개인의 행동과 직관에 내재되어 있음
• 개인에게 체화되어 있어 외부로 표출하거나 전달하기 어려움
• 문서화 또는 구조화가 어렵고, 주로 관찰, 시연, 경험적 학습을 통해 전수됨

② 형식지(Explicit Knowledge)
• 교과서, 매뉴얼, 비디오, 데이터베이스(DB)와 같이 형상화되고 구조화된 지식을 의미함
• 유형의 자료로 표현되어 있어 누구나 쉽게 이해하고 사용할 수 있음
• 지식의 전달과 공유가 쉬움
• 문서화, 데이터베이스화, 시스템화를 통해 조직 내 활용 가능

③ 콘텐츠와 지식의 관계
• 콘텐츠는 다양한 매체(동영상, 웹사이트, SNS 등)를 통해 광범위한 대상에게 전달 가능
• 콘텐츠는 지식이 특정 상황이나 맥락에 맞게 변형되고 활용된 형태라고 할 수 있다.

16 | 시험에 출제되는 핵심 내용 요약 정리

페이지 17
============================================================

004 암묵지와 형식지의 상호 작용★★

• 암묵지와 형식지의 상호작용은 지식을 창출하고 공유하며, 이를 조직의 학습 및 혁신으로 연결하는 핵심 과정이다. 이 과정은 다음 네 가지 단계로 구분

① 공통화(Socialization)
• 암묵지로부터 암묵지로 지식을 다른 사람과 공유하여 새로운 암묵지를 생성하는 과정

② 표출화(Externalization)
• 암묵지를 형식지로 전환하는 과정. 개인의 경험, 직감을 문서화하거나 시각적 자료로 표현하여 공유 가능하게 만듦

③ 연결화(Combination)
• 기존의 형식지를 조합하거나 새로운 지식을 추가하여 더 발전된 형식지로 확장하는 과정
• 데이터를 통합하고, 자료를 재구성하여 새로운 관점이나 정보를 생성함

④ 내면화(Internalization)
• 형식지를 학습하여 개인의 암묵지로 체화하는 과정
• 문서, 매뉴얼, 교육 자료 등을 기반으로 학습하여 실무에 적용하고 경험을 통해 내재화함

005 데이터와 정보의 관계★★

• 데이터와 담위적 특성인 근거의 기능에 주목하여, 데이터와 정보의 관계를 이해하기 위해 DIKW 피라미드를 정의할 필요가 있다.
• DIKW 피라미드란 Data → Information → Knowledge → Wisdom의 과정을 통해 데이터를 시작으로 최종적으로 지혜에 도달하는 과정을 나타냄

① Data(데이터)
• 가공되지 않은 순수한 수치나 기호, 특정 맥락 없이 단순히 존재하는 상태
예 인원 가격: A 마트 100원, B 마트는 200원

② Information(정보)
• 데이터를 가공하고 상관관계를 분석하여 의미를 부여한 상태
• 패턴을 인식하고, 데이터를 특정 맥락에서 해석 가능
예 "A 마트의 인원 가격이 B 마트보다 더 싸다."

③ Knowledge(지식)
• 서로 연결된 정보 간의 패턴을 이해하고, 이를 바탕으로 의사결정이나 예측을 도출한 상태

• 정보의 활용을 통해 실질적인 행동을 가능하게 함
예 "A 마트에서 장보면 좋다. A 마트에서 인원을 사거나."

④ Wisdom(지혜)
• 근본 원리와 맥락에 대한 깊은 이해를 바탕으로 통찰적이고 창조적인 아이디어를 도출한 상태
예 "A 마트의 인원이 저렴하나, 다른 상품도 B 마트보다 저렴한지 고려하여 우선적으로 고려해야겠다."

006 데이터베이스 정의★

① 데이터베이스의 정의
• 데이터베이스는 동시에 복수의 적용 업무를 지원하며, 다수 이용자의 요구를 충족하고 지정 및 정리가 위해 일정한 구조에 따라 조직된 데이터의 집합이다.
• 구조적 저장 : 데이터는 데이터, 혼, 명 등으로 구성되며, 명확한 관계에 스키마를 가진다.
• 독립적 존재 : 데이터베이스 자체는 데이터 저장소로 기능하며, 데이터를 저장하고 보존하는 데 초점이 맞춰져 있다.
• 응용프로그램 독립성 : 데이터베이스가 응용프로그램으로부터 독립되어 설계되고 관리된다는 것을 의미한다.

② 데이터베이스 관리시스템(DBMS) 정의
• DBMS는 데이터베이스를 효율적으로 관리한 활용할 수 있는 소프트웨어 도구로, 데이터 검색, 저장, 보안, 정합 복구 등의 기능을 제공한다.
• 중개 역할 : 데이터베이스와 사용자(또는 응용프로그램) 사이에서 데이터를 요청하고 처리하는 역할을 수행
• 데이터 검색 및 수정
• 사용자 관리 및 보안
• 데이터 무결성 유지 및 정합 복구

제1과목 데이터 이해 | 17

페이지 18
============================================================

2025 ADsP 한 권으로 끝내기

007 DBMS의 발전 과정★

• DBMS(Database Management System)는 데이터베이스 기술의 발전과 함께 여러 세대를 거쳐 발전

① 1세대 : 네트워크 DBMS, 계층 DBMS
• 데이터 구조를 네트워크형 또는 계층형으로 구성
• 구조가 복잡하며, 데이터의 변경이나 확장이 어려움

② 2세대 : 관계형(Relational) DBMS
• 데이터를 테이블 형태로 구성하여 직관적이고 간결한 데이터 관리가 가능
• SQL(Structured Query Language)을 통해 데이터 관리와 검색이 용이함

③ 3세대 : 객체 지향 DBMS 및 객체 관계형 DBMS
• 객체 지향(Object-Oriented) DBMS :
  - 멀티미디어 데이터와 같은 비정형 데이터를 처리하기 위해 등장
  - 같은 속성과 행위를 갖는 데이터를 클래스로 그룹화
  - 클래스 연산을 수행하기 위해 메소드(함수)를 정의함
  - 관계형 데이터 모델로 표현하기 어려운 복잡한 데이터를 효과적으로 관리
• 객체 관계형 모델(ORDBMS) :
  - 관계형 DBMS에 객체 지향 DBMS의 장점을 통합한 모델
  - 기존의 관계형 모델을 확장하여, 다양한 데이터 구조를 지원

④ 4세대 : NoSQL DBMS
• 데이터 구조를 미리 정의하지 않으므로 비정형 데이터를 저장하고 처리하는 데 적합
• 데이터의 분산 저장 및 대규모 데이터 처리에 유리함

008 데이터 유형 분류 ★★★

• 데이터는 구조화 정도와 저장 및 처리 방식에 따라 정형, 반정형, 비정형 데이터로 구분

① 정형 데이터(Structured Data)
• 정해진 형식과 구조를 가지며, RDBMS(Relational Database Management System)에 저장되는 데이터
• 고정된 필드와 스키마에 의해 데이터 구조가 정의
• SQL을 사용하여 데이터 검색과 관리가 용이함

② 반정형 데이터(Semi-Structured Data)
• 데이터의 논리적(메타데이터)를 표현하며, 정형 데이터와 비정형 데이터의 중간 형태
• 고정된 스키마는 없지만, 태그나 키-값 구조를 통해 데이터의 구조가 정의됨
• 일반적으로 파일 형태로 저장됨
예 HTML, JSON, XML 파일, 로그 데이터

③ 비정형 데이터(Unstructured Data)
• 정해진 형식이나 구조가 없으며, 일반적으로 대용량 데이터로 취급됨
• 이미지, 동영상, 오디오 등 복잡한 형태의 데이터
• 분석 및 처리가 까다로우며, 특수한 도구나 기술을 활용함
예 이미지 파일, 동영상 파일, 소셜 미디어 데이터

009 데이터베이스 설계★

① 요구 조건 분석
• 데이터베이스의 목적과 사용자의 요구 사항을 분석

② 개념적 설계(E-R 모델)
• 엔티티와 관계를 정의하여 데이터베이스의 전체적인 구조를 설계

③ 논리적 설계(데이터 설계)
• E-R 모델을 기반으로 데이터 구조를 설계하고 스키마를 정의

④ 물리적 설계(데이터 구조화)
• 데이터베이스를 실제 환경에서 구현하여, 저장소 및 성능을 최적화

010 데이터베이스의 특징★★

① 통합된 데이터(Integrated Data)
• 여러 데이터가 중복 없이 논리적으로 통합

② 저장된 데이터(Stored Data)
• 컴퓨터 시스템에 저장된 상태로 유지되는 데이터

③ 공용 데이터(Shared Data)
• 여러 사용자와 응용 프로그램이 공통으로 사용할 수 있음

④ 변화하는 데이터(Changed Data)
• 데이터는 시간의 흐름에 따라 추가, 삭제, 갱신이 발생함

18 | 시험에 출제되는 핵심 내용 요약 정리

페이지 19
============================================================

011 데이터웨어하우스(Data Warehouse)와 데이터 마트(Data Mart)★★★

• 데이터웨어하우스의 데이터 마트는 데이터 관리와 분석을 위한 저장소라는 공통점이 있지만, 규모와 용도에서 차이가 있다.

① 데이터웨어하우스(Data Warehouse)
• 기업의 모든 데이터를 통합 정리화하여 통합 저장하고, 의사결정을 지원하기 위한 대규모 데이터 저장소
• 통합된 데이터 : 여러 출처에서 수집한 데이터를 하나의 시스템에 통합
• 주제 중심적 : 조직 전체에서 사용할 수 있도록 데이터가 설계됨
• 시간 가변적 : 과거 데이터와 현재 데이터를 모두 포함하여 분석 가능
• 비휘발성 : 데이터를 저장한 후에는 변경하지 않으며, 분석을 목적으로만 사용
• 규모 : 대규모의 데이터를 다룸
• 사용자 : 기업의 전사적 의사결정을 담당하는 사용자

② 데이터 마트(Data Mart)
• 데이터웨어하우스의 데이터를 특정 부서나 비즈니스 영역에 최적화하여 제공하는 소규모 데이터 저장소
• 단일 주제 : 특정 부서나 비즈니스 영역예 마케팅, 판매에 초점
• 독립적 운영 가능 : 데이터웨어하우스 없이 독립적으로 운영되기도 함
• 규모 : 데이터웨어하우스보다 작고 간단함
• 사용자 : 특정 부서나 팀의 실무자

012 OLTP vs OLAP★★

① OLTP(On-Line Transaction Processing)
• 네트워크 상에서 여러 사용자가 실시간으로 데이터베이스의 데이터를 갱신하거나 조회하는 단위 작업을 처리하는 시스템
• 실시간 처리 : 즉 단위 또는 실시간으로 데이터를 입력하고 수정
• 데이터 무결성 유지 : 정확성과 일관성이 중요한 시스템
예 은행에서 고객이 ATM을 통해 입출금을 요청하면 OLTP 시스템이 실시간으로 처리

② OLAP(On-Line Analytic Processing)
• 데이터베이스에서 수집된 데이터를 분석하여 의사결정에 활용할 수 있는 정보를 도출하는 시스템
• 비실시간 처리 : 실시간이 아닌 주기적으로 데이터 분석
• 대규모 데이터 : 과거 데이터와 현재 데이터를 통합하여 처리
예 회사의 연간 판매 데이터를 분석하여 지역별 매출 트렌드를 파악

013 CRM과 SCM★★

• CRM과 SCM은 2000년대부터 기업 DB 구축 및 운영의 핵심 화두로 등장한 두 가지 주요 솔루션

• 두 시스템은 기업의 내부 운영과 외부 고객 관리에서 각각 중요한 역할을 담당하며, 서로 일치하게 연계되어 활용

① CRM(Customer Relationship Management)
• 고객과의 관계를 관리하고 강화하여 장기적인 고객 유지 및 수익 창출을 목표로 하는 솔루션
• 고객 데이터를 기반으로 고객의 요구를 파악하고 맞춤형 서비스를 제공

② SCM(Supply Chain Management)
• 제조, 물류, 유통 등 공급망의 모든 과정을 최적화하고, 정보기술을 활용해 재고 관리와 비용 절감을 목표로 하는 솔루션

제1과목 데이터 이해 | 19

페이지 20
============================================================

014 ERP와 BI와 BA★★

• ERP, BI, BA는 기업의 데이터 관리와 의사결정을 지원하는 핵심 기술
• 각각의 목적과 기능을 다르지만, 상호 보완적으로 사용되어 조직의 경쟁력을 높이는 데 기여

① ERP(Enterprise Resource Planning)
• 기업의 핵심 비즈니스 프로세스를 통합적으로 관리하는 시스템
• 제조, 구매, 제고, 주문, 공급망, 고객 서비스 등 다양한 프로세스를 지원

② BI(Business Intelligence)
• 데이터를 기반으로 의사결정을 지원하기 위해 설계된 리포트 중심 도구
• 과거 데이터를 분석하여 현재의 상태를 시각적으로 이해하고 판단할 수 있도록 조력

③ BA(Business Analytics)
• 빅데이터 고급 분석 기술을 적용하여 미래를 예측하나, 통계 전략의 결과를 예측하는 도구
• 통계적 및 수학적 모델링과 머신러닝을 활용한 데이터 분석

015 기타 기업 내부 데이터베이스 솔루션★

① EAI(Enterprise Application Integration)
• 기업 애플리케이션 통합 솔루션으로, 기업 내 다양한 시스템(ERP, CRM, SCM 등)을 상호 연동하여 통합된 정보 체계를 구축하는 것을 지원

② 블록체인(Blockchain)
• 데이터 분산 저장 기술로, 네트워크에 참여하는 모든 사용자가 거래 데이터를 분산 저장하여 데이터의 위·변조를 방지
• 블록과 체인 구조로 이루어져, 데이터의 투명성과 보안을 보장
• 중앙 서버가 아닌 네트워크 참여자들이 데이터를 공유 및 저장
• 중앙 서버가 없어 특정 서버 해킹으로 인한 데이터 손실이 방지됨

016 차세대바이 데이터베이스 솔루션

① 종합통류정보망
• 운행 차량의 위치 및 상태를 실시간으로 추적하여 운송회사와 서비스 가입자의 협력적 의사결정을 지원

② 부가가치통신망(VAN, Value Added Network)
• 통신사업자가 부가가치를 창갈한 정보 전송 네트워크로, 정보를 축적, 가공, 변환하여 제공

③ 국가지리정보체계(NGIS, National Geographic Information System)
• 국가 및 지역별 지리 데이터를 관리하고 활용하는 정보체계

④ 지능형교통시스템(ITS, Intelligent Transportation System)
• 교통 상황 데이터를 실시간으로 수집하고, 이를 활용해 교통 흐름을 최적화하는 시스템

⑤ 의료 EDI(Electronic Data Interchange)
• 병원, 약국, 보험사 간 전자적으로 의료 정보를 교환하는 시스템

⑥ 교육행정정보시스템(NEIS, National Education Information System)
• 전국 초·중·고등학교의 교육 행정 데이터를 통합 관리하는 시스템

017 빅데이터(Big Data)의 5V 특징★

• 빅데이터는 데이터 관리 및 분석에서 중요한 개념으로, Volume(규모), Variety(다양성), Velocity(속도), Value(가치)가 5가지 핵심 특징인 정의

① Volume(데이터의 크기)
• 방대한 양의 데이터 규모를 의미하며, 생성되는 모든 데이터의 수집고 저장하는 것을 목표

② Variety(데이터의 다양성)
• 데이터의 형식과 구조가 다양하며, 정형, 반정형, 비정형 데이터를 포함

③ Velocity(데이터의 속도)
• 데이터가 생성되고, 처리되고, 분석되는 속도를 의미

④ Value(데이터의 가치)
• 방대한 데이터를 분석하여 유용한 정보와 통찰을 도출하는 것

⑤ Veracity(데이터의 정확성)
• 데이터의 신뢰성과 정확성을 의미하며, 데이터 품질 관리가 중요

20 | 시험에 출제되는 핵심 내용 요약 정리

페이지 21
============================================================

018 빅데이터가 통찰하게 된 결정적 요인★

• 빅데이터의 등장은 기술, 조직, 사회적 변화 등 여러 요인이 복합적으로 작용하여 가능

① 기술적 변화
• 대량의 데이터를 저장하고 처리하기 위해 고가의 서버와 데이터센터를 수유하지 않고도, 클라우드 기반 플랫폼을 통해 저비용으로 데이터 관리 가능
• 하둡(Hadoop)과 같은 분산 데이터 저장 기술 등장으로 대규모 데이터를 효율적으로 저장하고 처리 가능

② 인재 및 조직적 변화
• 데이터의 폭발적 증가로 인해 이를 분석하고 통찰을 도출할 수 있는 데이터 사이언티스트의 중요성이 대두

019 빅데이터 출현 배경★★

• 빅데이터는 완전히 새로운 현상이 아니라, 기존 데이터와 기술, 그리고 이를 다루는 사람과 조직의 변화가 축적되어 나타난 결과

① 산업계 · 학술 진원 변화
• 데이터를 양적으로 많이 축적하다 보면, 어느 순간 이를 활용해 정적으로 새로운 가치를 창출할 수 있는 전환점이 도래한다는 개념

② 학계 · 빅데이터를 다루는 현상의 증가
• 빅데이터는 학문적으로도 새로운 분석 도구와 방법론의 필요성을 가져왔고, 이를 연구하는 사례가 증가

③ 관련 기술 발전
• 디지털화와 기술 발전, 특히 클라우드 컴퓨팅, 저장 기술, 인터넷 및 모바일 기술이 빅데이터 시대를 열었다.
• 이미지, 동영상, 소셜미디어 텍스트 등 기존의 정형 데이터와는 다른 데이터 유형의 급증

020 빅데이터의 기능★

① 빅데이터는 현조에 비유
• 빅데이터는 세상을 분석하고 이해하는 새로운 시각을 제공하며, 데이터를 통해 보이지 않던 패턴과 통찰을 발견할 수 있음
• "렌즈"는 데이터 분석 도구나 시각화를 통해 세상을 이해하는 방식을 상징

② 빅데이터는 플렛폼에 비유
• 빅데이터는 다양한 지원에서 활용 가능하며, 이를 기반으로 새로운 비즈니스와 생태계를 구축할 수 있는 기반 인프라 역할을 함
• "플렛폼"은 공통 활용을 목적으로 구축된 구조물로, 이를 통해 데이터의 유통, 공유, 분석이 이루어짐
예 페이스북 : 사용자 데이터를 기반으로 광고 플랫폼을 운영체제(OS) : 다양한 앱과 기능을 지원하는 기본 구조물

021 빅데이터가 만들어 내는 본질적 변화★★★

• 빅데이터의 등장과 활용은 데이터 처리 방식, 분석 방법, 그리고 의사결정 패러다임에 근본적인 변화를 가져옴

① 정보의 사전처리에서 사후처리 시대로
• 과거 : 데이터를 분석하기 전에 필요한 정보를 선별하고 처리하는 방식이 일반적이었음
• 현재 : 데이터를 모든 수집한 후 분석하며 저장 비용이 감소하면서 데이터의 선별 없이 대규모 데이터 분석이 가능해짐

② 표본조사에서 전수조사로
• 과거 : 표본조사를 통해 제한된 데이터만 분석하여 전체 경향성을 추정
• 현재 : 빅데이터 기술로 인해 전수조사가 가능해져 정확성과 신뢰도가 향상됨

③ 질보다 양으로
• 과거 : 데이터의 품질에 초점을 맞추어 제한된 데이터로 분석
• 현재 : 데이터의 양이 많아질수록 더 정밀한 결과를 도출할 수 있다는 패러다임 변화

④ 인과관계에서 상관관계로
• 과거 : 데이터를 분석하여 인과관계를 도출하려는 시도가 중심이었음
• 현재 : 빅데이터 분석에서는 상관관계를 찾아내는 데 초점을 맞춤

제1과목 데이터 이해 | 21

페이지 22
============================================================

022 빅데이터의 가치 산정이 어려운 이유★

• 빅데이터는 방대한 양, 다양한 형식, 높은 처리 속도 등의 특성으로 인해 그 가치 산정이 쉽지 않다.

① 데이터 활용 방식
• 재사용 및 다목적 활용 : 데이터는 한 번 사용되고 끝나는 것이 아니라 다양한 목적으로 활용

② 데이터의 가족에 없던 가치 창출
• 데이터는 기존에 정의되지 않은 방식으로 새로운 가치를 창출

③ 분석 기술 발달의 영향
• 새로운 기술로 데이터 가치 창출

023 빅데이터 활용 대표 사례★

• 빅데이터는 기업, 정부, 개인 등 다양한 주체가 각자의 목적에 맞게 데이터를 활용하여 새로운 가치를 창출

① 기업 활용 사례
• 구글 검색 : 검색 로그 데이터를 활용하여 기존 페이지랭크 알고리즘을 개선
• 월마트 구매 패턴 분석 : 연관 규칙 분석을 통해 고객의 구매 패턴을 파악
• IBM 왓슨 : 인공지능 시스템을 활용해 의료 데이터를 분석하여 진단 및 치료 방안을 제시

② 정부 활용 사례
• 환경 탐사 : 실시간 교통정보 수집, 기후 정보 분석
• 상황 분석 : 소셜미디어 분석, CCTV 및 통화 기록

③ 개인 활용 사례
• 정치인의 SNA(Social Network Analysis) 활용
• 가족 관들의 청취 분석

024 빅데이터 활용 기법★★★

• 빅데이터 분석에서는 다양한 기법이 활용되며, 각각의 기법은 특정한 문제를 해결하거나 패턴을 발견하는 데 사용된다.
• 아래는 대표적인 빅데이터 활용 기법과 그 적용 사례이다.

① 연관규칙학습(Association Rule Learning)
• 데이터에서 변수 간 상관관계를 찾아내는 방법으로 어떤 항목이 함께 발생할 가능성이 높은지를 분석
예 마트에서 상관관계가 높은 상품을 함께 진열, 온라인 쇼핑몰에서 고객의 구매 패턴 분석

② 유형 분석(Classification Tree Analysis)
• 데이터를 특정 기준에 따라 그룹화하거나 분류하는 데 사용
• "사용자가 어떤 특성을 가진 집단에 속하는가?"와 같은 질문에 답을 제공
예 온라인 수강생들의 특성에 따라 학습 그룹 분류

③ 유전 알고리즘(Genetic Algorithms)
• 자연선택과 유전학 원리를 활용하여 최적화 문제를 해결하는 방법
예 최대의 시정률을 얻기 위한 방송 시간대 최적화

④ 기계 학습(Machine Learning)
• 데이터에서 학습하고 패턴을 발견하여 미래를 예측하거나 의사결정을 지원하는 알고리즘
• 출현 데이터를 기반으로 모델을 생성
예 넷플릭스의 영화 추천 시스템, 전자상거래에서 고객 맞춤형 상품 추천

⑤ 회귀분석(Regression Analysis)
• 독립변수와 종속변수 간의 관계를 파악하여 변수 간 상관성을 분석
예 구매자의 나이가 구매 차량 타입에 미치는 영향 분석

⑥ 감성분석(Sentiment Analysis)
• 텍스트 데이터에서 감정적, 부정적 감정을 파악하여 의견과 평가를 분석
예 새로운 활용 정책에 대한 고객의 평가는 어떤가?

⑦ 소셜 네트워크 분석(Social Network Analysis)
• 소셜 네트워크 데이터를 분석하여 연결성과 영향력있는 인물을 식별
예 소셜커머스에서 고객 간의 관계와 영향력 분석

22 | 시험에 출제되는 핵심 내용 요약 정리

페이지 23
============================================================

025 빅데이터 시대의 위기 요인과 통제 방안★★★

• 빅데이터 시대는 정보와 기술의 발전으로 많은 기회를 제공하지만, 동시에 다양한 윤리적, 사회적 위기를 초래할 수 있다.

① 사생활 침해(Privacy Violation)
• 위기 요인
  - 정보 수집 센서(M2M)의 증가
  - 데이터의 2차, 3차 활용 가능성 증가
  - 사생활 침해와 사회적 위험
• 통제 방안
  - 동의에서 철회으로 전환 : 개인이 데이터 동의했어 하는 현재의 시스템은 비효율적이며, 데이터 사용자가 더 큰 책임을 져도록 하는 체계가 필요

② 책임 윤식의 훼손(Erosion of Accountability)
• 위기 요인
  - 빅데이터 분석으로 개인의 행동, 성향, 위험 요인 등이 정보가 발전
  - 예측 결과만으로 개인을 단정하거나 제품을 가하는 사회로 변질될 가능성
  - 이는 개인의 기본권 침해와 민주주의 원칙 훼손으로 이어질 수 있음
• 통제 방안
  - 책임 윤식 보장 : 예측 결과만으로 처벌이나 불이익을 주는 것을 금지하고, 투명성과 정당성을 확보

③ 데이터의 오용(Misuse of Data)
• 위기 요인
  - 빅데이터는 과거에 없어난 사건과 데이터를 기반으로 하고 예측하는 데는 한계가 있으며, 새로운 상황이나 변수를 반영하지 못할 수 있음
• 통제 방안
  - 알고리즘 대상 정급권 확보 : 데이터 알고리즘의 실제와 작동 방식에 대한 투명성 확보
  - 관련권 인증 방안 도입 : 알고리즘이 편향 없이 설계되었는지, 신뢰할 수 있는 결과를 도출하는지 검증하는 공인 인증 시스템 구축
  - 알고리즘 전문가 역할(알고리즘니스트) : 객관과 투명성 강화

026 데이터 3법★

• 데이터 3법은 데이터의 이용을 활성화하면서도 개인 정보를 보호하기 위해 마련된 법률로, 다음의 세 가지 법률을 포함

① 개인정보보호법
② 정보통신망 이용촉진 및 정보보호 등에 관한 법률(정보통신망법)
③ 신용정보의 이용 및 보호에 관한 법률(신용정보법)

027 개인정보 비식별화(De-identification of Personal Information)★★

• 개인정보 비식별화는 개인정보를 특정 개인과 연관 지을 수 없도록 처리하는 방법으로, 데이터 활용과 개인정보 보호를 동시에 달성하기 위해 중요하게 다뤄진다.

① 개인정보화
• 삭어 있는 개인에 관한 정보로, 특정 개인을 직접적 또는 간접으로 식별할 수 있는 정보를 의미

② 비식별화란
• 정보의 일부 또는 전부를 삭제하거나 대체하여 특정 개인을 식별할 수 없도록 하는 조치
• 데이터를 다른 정보와 결합하더라도 개인 식별이 어렵도록 처리

③ 개인정보 비식별화 기술

| 비식별기술 | 세기방법 | 예시 |
|----------|----------|------|
| 가명처리 | 식별요소를 다른 값으로 대체 | 홍길동, 35세, 서울 거주, 한국대 재학 → 임영성, 30대, 서울 거주, 국제대 재학 |
| 총계처리 | 데이터를 총합으로 표시하여 개별 데이터가 될 보이지 않도록 대체 | 임영성 180cm, 출길동 170cm → 1-5번 학생 키 합 350cm, 평균 키 175cm |
| 데이터값 삭제 | 개인 식별을 위한 특징 수 있는 값 삭제 | 홍길동, 35세, 서울 거주, 한국대 출현 → 35세, 서울 거주 |
| 범주화 | 범주의 값으로 변환 | 홍길동, 35세 → 홍씨, 30-40세 |
| 데이터 마스킹 | 개인 식별가능 정보를 처리 | 홍길동, 35세 → 홍**, 35세 |

제1과목 데이터 이해 | 23

페이지 24
============================================================

028 빅데이터의 활용에 필요한 3요소★

• 빅데이터의 성공적인 활용을 위해서는 데이터, 기술, 인력이라는 세 가지 핵심 요소가 필요

① 데이터(Data)
• 빅데이터 활용의 핵심 자원으로, 다양한 형태와 방대한 양의 데이터를 의미
• 데이터를 수집, 저장, 가공하는 과정에서 모든 현상을 데이터화(Datafication) 하는 것이 중요

② 기술(Technology)
• 데이터를 처리하고 분석하기 위한 도구와 기술로, 빅데이터의 성격적 분석을 가능하게 함

③ 인력(People)
• 데이터를 분석하고, 기술을 활용하여 가치를 창출할 수 있는 전문 인력이 필요
• 데이터 사이언티스 : 데이터를 분석하고 모델을 설계하여 비즈니스 문제를 해결
• 알고리즘이스트(Algorithmist) : 알고리즘 설계와 검토, 공정성 보장을 담당, 데이터 분석 피해자가 제기한 문제를 조사하고, 결투된 결과를 조정하거나 수정

029 빅데이터 영향과 회의론★★

• 빅데이터의 중요성과 활용이 강조되는 가운데, 이에 대한 성과와 기대가 항상 일치하지 않으며, 빅데이터 회의론이 대두되고 있다.
• 빅데이터 분석이 성공하려면 단순히 데이터의 양과 기술적 최적화에 의존하기보다는, 의미 있는 통찰과 전략적 가치를 도출하는 데 초점을 두어 한다.

① 빅데이터 분석의 성과 창출 관련
• 빅데이터 분석의 목적은 데이터에서 통찰(Insight)을 도출하여 의사결정과 비즈니스 성과를 창출하는 것
• 하지만 복잡한 빅데이터를 최적화하는 기술만으로는 최고의 가치 창출할 수 없음

② 데이터 크기보다 통찰력에 집중
• 빅데이터는 단순히 크기(Volume)가 중요한 것이 아니라, 데이터를 통해 의미 있는 시각과 통찰을 도출하는 것이 관건
• 빅데이터 활용의 잠재올 비용 문제보다 분석 방법과 성과에 대한 이해 부족에 있음

③ 성과와 데이터 기반 의사결정을 한계
• 성과가 높은 기업일수록 데이터 기반 의사결정을 체계화하고, 전략적 통찰을 놓이고 있었음

030 일차적인 분석 vs 전략 도출을 위한 가치 기반 분석★

• 데이터 분석은 일차적인 분석에서 시작하지만, 기업의 성과와 지속 가능성을 높이기 위해서는 전략적 인사이트를 제공하는 가치 기반 분석으로 나아가야 한다.

① 일차적인 분석
• 일차적인 분석은 주로 특정 부서나 팀의 업무 개선을 목표로 하는 분석으로, 비교적 좁은 범위와 내부 문제에 초점을 둠

② 전략 도출을 위한 가치 기반 분석
• 가치 기반 분석은 조직의 경쟁력을 강화하기 위해 전략적 인사이트를 도출하는 데 초점을 둠 분석
• 업계 내부 데이터만만 아니라, 인구통계학적 변화, 경제사회 트렌드, 고객 니즈의 변화 등 외부 요인까지 고려
• 전략적 가치기반 분석은 기존 성과를 유지하는 데 초점이 아니라, 비즈니스 성과를 개선하고 혁신하는 것이 주된 목적

24 | 시험에 출제되는 핵심 내용 요약 정리

페이지 25
============================================================

031 데이터 사이언스 vs 데이터 마이닝 vs 통계학★★

• 데이터를 다루는 세 가지 주요 학문인 데이터 사이언스, 데이터 마이닝, 통계학은 서로 밀접하게 관련되어 있지만, 목적, 접근 방식, 분석 대상에서 차이가 있다.

① 데이터 사이언스(Data Science)
• 데이터를 활용하여 정량 있는 정보를 추출하고 이를 효과적으로 구현하여 전달하는 학문
• 데이터 분석뿐 아니라 데이터 관리, 모델 개발, 시각화, 전달까지 모든 과정을 포함

② 데이터 마이닝(Data Mining)
• 데이터에서 패턴과 관계를 탐색하고, 이를 통해 유용한 정보를 추출하는 기술과 과정
• 주로 분석에 초점을 둠

③ 통계학(Statistics)
• 데이터를 수집, 분석, 해석, 표현하는 방법론을 연구하는 학문
• 주로 정형화된 데이터와 실험 데이터를 대상으로 함

032 데이터 사이언스의 핵심 구성 요소★

① IT(Data Management)
• 데이터를 수집, 저장, 관리, 처리하는 기술과 인프라를 의미

② Analytics(분석적 영역)
• 데이터를 분석하여 패턴을 발견하고 예측 모델을 구축하는 과정

③ 비즈니스 분석
• 데이터 분석 결과를 실제 비즈니스 문제 해결과 의사결정 지원에 활용

033 데이터 사이언티스트가 갖춰야 할 역량 (T자형 기능)★

• 데이터 사이언티스트는 데이터 분석과 관련된 기술적 능력뿐만 아니라, 비즈니스 이해와 소프트 스킬을 겸비해야 한다.
• 가트너(Gartner)가 제시한 데이터 사이언티스트의 주요 역량

① 데이터 관리
• 데이터를 수집, 정리, 저장, 가공하는 능력으로, 데이터의 구조적 선적을 깊이 이해해야 함

② 분석 모델링
• 데이터를 분석하고 통계적, 머신러닝 모델을 설계하여 예측 분석하는 능력

③ 비즈니스 분석
• 분석 결과를 활용해 비즈니스 문제를 해결하고, 조직의 전략적 목표를 지원하는 능력

④ 소프트 기능
• 데이터 분석 결과를 조직 효과적으로 전달하고, 협업을 통해 프로젝트를 성공적으로 이끄는 능력

034 데이터 사이언티스트 요구 역량 (Hard Skill & Soft Skill)★★★

• 데이터 사이언티스트는 하드 스킬(기술적 역량)과 소프트 스킬(비기술적 역량) 모두 갖춰야 하며, 소프트 스킬
• 하드 스킬은 데이터 분석 기술의 전문성을, 소프트 스킬은 분석 결과를 활용해 조직 내 가치를 창출하는 능력을 나타낸다.

① Hard Skill
• 빅데이터에 대한 이론적 지식
• 분석 기술에 대한 숙련
• 최적화 설계 및 노하우 축적

② Soft Skill
• 통찰력 있는 분석 : 창의적 사고, 논리적 비판, 호기심
• 설득력 있는 전달 : 분석 결과를 효과적으로 전달하고 의사결정을 지원할 수 있는 스토리텔링과 시각화 능력
• 다분야 간 협력 : 분석 과정에서 다양한 팀과 협력하고, 커뮤니케이션을 통해 조화를 이루는 능력

035 가치 패러다임의 변화★★

① 디지털화(Digitalization)
• 아날로그 정보를 디지털로 전환하여 저장, 처리, 활용 가능하게 만드는 과정

② 연결(Connection)
• 개별 디지털화 정보와 대상이 서로 연결됨으로써 가치가 증폭되는 단계

③ 에이전시(Agency)
• 연결된 환경에서 복잡한 관계를 관리하고, 시스템이 자율적으로 작동하도록 지원

제1과목 데이터 이해 | 25

페이지 26
============================================================

2과목 데이터의 분석 기획

036 분석 주제 유형★★

• 분석의 대상(what)과 방법(how)에 따라 4가지 유형으로 구분

① Optimization(최적화)
• 정의 : 분석 대상과 방법이 명확하며, 문제를 최적화하여 해결하는 방식
• 목표 : 효율성 극대화, 비용 최소화, 성능 개선

② Solution(솔루션)
• 정의 : 분석 대상은 명확하지만, 구체적인 분석 방법이 불분명한 때 해결책을 찾는 방식
• 목표 : 문제 해결 및 절실한 성과 도출

③ Insight(통찰)
• 정의 : 분석 방법은 알고 있으나 분석 대상이 불명확한 경우 새로운 통찰을 얻는 방식
• 목표 : 데이터에서 의미 있는 패턴과 관계 도출

④ Discovery(발견)
• 정의 : 분석 대상과 방법 모두 불명확한 상황에서 새로운 데이터와 관계를 탐구
• 목표 : 분석 대상 자체를 새롭게 정의하고, 데이터 기반 문제를 새롭게 발견

037 목표 시점별 기획 방안★

• 분석 및 기획 방안은 목표 시점에 따라 단기적인 과제 중심 접근과 장기적인 마스터플랜 접근으로 나눈다.
• 각각의 방식은 해결해야 할 과제의 긴급성과 지속 가능성에 따라 선택되며, 당면한 문제 해결과 분석 문화 내재화라는 상반된 목표를 담당해야 한다.

| 구분 | 과제 중심적 접근 방식 | 장기적인 마스터플랜 방식 |
|------|---------------------|------------------------|
| 목표 | 당면한 분석 주제를 빠르게 해결 | 지속적이고 체계적인 분석 문화 내재화 |
| 분석 단위 | 과제 단위 | 마스터플랜 단위 |
| 핵심 가치 | Speed & Test | Accuracy & Deploy |

038 분석기획 시 고려사항★★

• 효율적이고 성공적인 분석기획을 위해 데이터, 유스케이스, 장애요소 포함한 다양한 요소를 사전에 고려해야 한다.

① 가용한 데이터(Available Data)
• 데이터의 존재성, 반정형, 비정형에 따라 적용 가능한 분석 방법이 달라지므로, 데이터를 이해하는 것이 첫 번째 단계

② 적절한 유스케이스(Proper Use-Case) 탐색
• 비즈니스 목표와 분석 목표를 일치시키기 위해 유사한 분석 사례(Use-Case)를 탐색
• 유스케이스는 분석의 가이드라인 역할을 하며, 기존 솔루션과 방법론 활용 가능

③ 장애요소(Barriers) 사전 제거
• 분석 프로젝트에서 발생할 수 있는 잠재적 장애요소를 미리 파악
• 장애요소에 대한 사전 대응책을 마련하여 프로젝트 진행이 차질이 없도록 함

039 데이터 저장 방식★★

• 데이터 저장 방식은 데이터의 구조와 사용 목적에 따라 크게 RDB(Relational Database)와 NoSQL로 나눈다.

① RDB(Relational Database)
• 정의 : 관계형 데이터베이스로, 데이터를 테이블 형식으로 저장하며 테이블 간 관계를 정의
• SQL 사용으로 표준화된 데이터 관리 가능
• 주요 도구(Oracle, MySQL, MSSQL)

② NoSQL
• 정의 : 비정형 데이터, 반정형 데이터 등 구조화되지 않은 데이터를 저장하고 처리하기 위한 비관계형 데이터베이스
• 대규모 데이터 처리와 분석 저장에 최적화
• 데이터 모델링이 자유롭고 유연성 제공
• 주요 유형 및 도구(MongoDB, Redis, Cassandra, HBase)

| 과제 유형 | Quick-Win | Long-Term View |
|----------|-----------|----------------|
| 접근 방식 | Problem Solving | Problem Definition |

26 | 시험에 출제되는 핵심 내용 요약 정리

페이지 27
============================================================

040 기업의 협리적 의사결정 저해 요소★

① 고정관념(Stereotype)
• 특정 사물, 상황, 사람에 대한 고정된 신념이나 이미지로 인한 새로운 정보를 왜곡하거나 거부

② 편향된 생각(Bias)
• 개인적 경험, 감정, 선호도 인해 객관적 데이터나 사실을 왜곡하는 경향

③ 프레이밍 효과(Framing Effect)
• 문제의 표현 방식에 따라 동일한 사건이나 상황에 대한 판단이나 선택이 달라지는 현상

041 분석 방법론의 구성 요소★

① 상세한 절차(Procedure)
• 분석 수행을 위한 단계적 가이드라인으로, 직업의 체계성과 일관성을 유지
예 데이터 수집 → 전처리 → 이를 예측 모델링 → 결과 해석

② 방법(Methods)
• 분석 문제 해결을 위한 구체적인 접근 방식
예 통계적 분석, 기계 학습

③ 도구와 기법(Tools & Techniques)
• 분석을 수행하기 위한 소프트웨어 및 알고리즘
예 Python/R,Tableau/Power BI

④ 템플릿과 산출물(Templates & Outputs)
• 분석 과정의 표준화를 돕는 템플릿과 결과물을 문서화한 자료
예 분석 계획서

042 폭포수(Waterfall), 나선형(Spiral), 프로토타입(Prototype) 모델 비교★★

• 소프트웨어 개발 및 프로젝트 관리에서 사용되는 폭포수 모델, 나선형 모델, 프로토타입 모델은 각기 다른 특징과 적합한 상황을 가진다.

① 폭포수 모델(Waterfall Model)
• 프로젝트를 단계별로 진행하며, 각 단계가 완료되고 승인된 후에만 다음 단계로 넘어가는 방식
• 장점
  - 각 단계가 명확히 정의되어 있어 체계적이고 이해하기 쉬움
• 단점
  - 단계별 진행으로 변경사항 적용이 어렵고 비용이 증가
  - 반복과 피드백이 제한적

② 나선형 모델(Spiral Model)
• 여러 번의 개발 반복을 통해 점진적으로 프로젝트를 완성하는 방식
• 장점
  - 위험 관리가 용이하며, 반복을 통해 점진적으로 품질 향상
  - 초기에 요구사항이 명확하지 않아도 진전식으로 구체화 가능
  - 대규모 프로젝트에서 유용
• 단점
  - 관리가 복잡하며, 반복 제대로 관리하지 못하면 비용과 일정 초과
  - 피드백과 일정 조과

③ 프로토타입 모델(Prototype Model)
• 초기 단계에서 사용 가능한 프로토타입(시제품)을 개발하여 사용자로부터 피드백을 받고, 이를 바탕으로 반복적으로 개선하는 방식
• 장점
  - 사용자의 요구사항을 명확히 파악할 수 있음
  - 초기 오류를 신속히 발견하고 개선 기능
  - 반복적으로 개선하나가는 개발 방식으로 개발 시간을 단축하는 데 유리
• 단점
  - 지속적인 피드백 과정으로 시간과 비용 증가

제2과목 데이터의 분석 기획 | 27

페이지 28
============================================================

043 KDD 분석절차(Knowledge Discovery in Databases)★★★

• KDD는 데이터베이스에서 유용한 지식을 발견하기 위한 체계적인 분석 절차
• 데이터 준비부터 분석 결과의 해석과 활용까지 포괄적인 단계를 포함하여, 데이터 마이닝 기법의 효과적인 적용을 지원

① 비즈니스 이해와 목표 설정
• 무엇을 분석하고, 어떤 결과를 원하는지 정의

② 데이터 선택
• 목적에 맞는 데이터 세트를 선정

③ 데이터 전처리(Preprocessing)
• 분석의 정확성과 신뢰성을 높이기 위해 데이터를 정리하고 준비하는 과정
• 잠음 처리 : 데이터에 포함된 불필요한 정보 삭제
• 결측값 처리 : 사라, 평균 대체, 예측값 대체
• 이상치(Outlier) 처리 : 데이터 입력 오류, 측정 오류를 대체

④ 데이터 변환(Transformation)
• 분석 목적에 맞게 데이터셋을 조정하거나 변경하는 단계
• 변수 선택(Feature Selection) : 분석 목적과 관련성이 높은 변수를 선택
• 차원 축소(Dimensionality Reduction) : 데이터의 차원을 줄여 분석 속도를 높이고 과적합(Overfitting)을 방지
• 데이터 스케일링(Scaling) : 데이터 간의 범위를 조정하여 모델 학습 과정에서 특정 변수의 영향을 과도하게 받지 않도록 함

⑤ 데이터 마이닝(Data Mining)
• 변환된 데이터셋을 활용하여 패턴을 발견, 데이터를 분류, 예측 작업을 수행하는 과정
• 데이터 마이닝은 다양한 기법과 알고리즘을 사용하여 비즈니스 목표에 맞는 분석 결과를 도출하는 데 중점

⑥ 데이터 마이닝 결과 평가(Interpretation / Evaluation)
• 데이터 마이닝 결과 평가 단계는 분석 결과를 해석, 평가, 활용하는 과정으로, 데이터 마이닝의 성패를 결정하는 중요한 단계

044 CRISP-DM 분석 방법론★

• CRISP-DM(Cross Industry Standard Process for Data Mining)은 다양한 산업 분야에서 데이터 마이닝 프로젝트를 체계적으로 수행하기 위해 설계된 표준 프로세스

• 6단계로 구성되어 있으며, 각 단계는 피드백을 통해 앞선도록 높이고 유연성을 제공

[CRISP-DM Process 다이어그램 포함]

28 | 시험에 출제되는 핵심 내용 요약 정리

페이지 29
============================================================

045 CRISP-DM 6단계★★★

① 업무 이해(Business Understanding)
• 업무 이해 단계는 데이터 마이닝 프로젝트의 목적과 요구사항을 비즈니스 관점에서 이해하고, 이를 데이터 분석 문제로 정의하는 기초 단계
• 업무 목적 파악 : 프로젝트의 비즈니스 목표를 명확히 정의
• 상황 파악 : 기존 프로세스 및 시스템 검토
• 데이터 마이닝 목표 설정 : 데이터 마이닝의 구체적 목표 정의(분류, 예측, 군집화 등)

② 데이터 이해(Data Understanding)
• 데이터 이해는 프로젝트에서 사용할 데이터를 수집, 탐색, 분석하여 데이터의 특성과 품질을 파악하고, 승자한 인사이트를 발견하는 단계
• 초기 데이터 수집 : 분석을 수행하기 위해 데이터를 수집하고 정리
• 데이터 기술 분석 : 데이터를 은아하고 기본적인 특성을 확인

③ 데이터 준비(Data Preparation)
• 데이터 준비 단계는 분석 기법에 적합한 데이터 세트를 구성하기 위해 수집된 데이터를 정리하고 변환하는 과정
• 분석을 데이터 세트 선택 : 분석에 필요한 데이터만 선택하여 세트를 구성
• 데이터 정제(Data Cleaning) : 데이터의 결함을 수정하여 품질을 개선
• 데이터 통합(Data Integration) : 서로 다른 소스에서 데이터를 결합하여 통합 데이터 세트를 생성
• 데이터 포맷팅(Data Formatting) : 데이터를 분석 기법에 적합한 형식으로 변환

④ 모델링(Modeling)
• 모델링 단계는 다양한 알고리즘과 기법을 선택하여 데이터를 분석하고, 모델을 구축, 평가, 최적화하는 과정
• 모델링 기법 선택 : 분석 목적과 데이터 특성에 따라 적합한 알고리즘과 기법을 선택
• 모델 테스트 계획 설계 : 모델의 성능을 검증하기 위한 테스트 데이터와 평가 기준을 설정

⑤ 평가(Evaluation)
• 평가 단계는 데이터 마이닝 과정에서 도출된 모델의 성능과 결과를 프로젝트의 목적에 부합하는지를 판단하는 단계
• 분석 결과 평가 : 모델이 생성한 결과가 비즈니스 목표를 효과적으로 지원하는지 확인
• 모델링 과정 평가 : 모델링 과정에서의 문제점이나 충족하지 확인

⑥ 전개(Deployment)
• 전개 단계는 모델링과 평가를 통해 완성된 모델을 실제 업무에 적용하고, 이를 운영 환경에서 안정적으로 활용하기 위해 계획을 수립하고 실행하는 단계
• 전개 계획 수립 : 모델을 실제 운영 환경에 적용하기 위한 실행 계획 수립
• 모니터링 및 유지보수 계획 수립 : 운영 중인 모델의 성능을 모니터링하고, 필요 시 업데이트
• 프로젝트 종료 보고서 작성 : 프로젝트의 전 과정과 성과를 요약한 보고서를 작성
• 프로젝트 검토 : 프로젝트 전체를 리뷰하고 성공 요인 및 개선점을 도출

제2과목 데이터의 분석 기획 | 29

페이지 30
============================================================

046 빅데이터 분석 방법론의 체계적 프로세스 모델★★

• 빅데이터 분석을 체계적으로 수행하기 위해 설계된 계층적 프로세스 모델(Stepwised Process Model)은 3계층(단계, 태스크, 스텝)으로 구성

① 단계(Phase)
• 분석 프로젝트의 최상위 계층으로, 프로세스 그룹(Process Group)을 기반으로 주요 목표를 달성하기 위한 흐름을 정의
• 각 단계는 기준선(Baseline)으로 설정되어 관리된다.
• 변경 사항은 버전 관리(Configuration Management)로 통제

② 태스크(Task)
• 단계를 구성하는 세부 활동 단위로, 각 태스크는 물리적 또는 논리적 작업을 정의
• 여러 태스크로 단계가 구성됨

③ 스텝(Step)
• 각 과업 분할한 세부 프로세스 단위로. WBS(Work Breakdown Structure)의 워크 패키지에 해당
• 각 스텝은 입력자료(Input), 처리 및 도구(Process & Tool), 출력자료(Output)로 구성
• 구체적이고 실행 가능한 작업

047 빅데이터 분석 방법론★★★

[빅데이터 분석 방법론 단계별 다이어그램]

분석 기획 → 데이터 준비 → 데이터 분석 → 시스템 구현 → 평가 및 전개

비즈니스 이해 및 범위 설정:
- 일반 데이터 준비
- 분석용 데이터 준비

프로젝트 정의 및 계획 수립:
- 데이터 스토어 개발
- 데이터 수집 및 저장법 개발

프로젝트 위험 계획 수립:
- 테스크 분석
- 분석결과 검증

데이터 스토어 설계:
- 데이터 준비 및 정제 작업 침하고 검증

30 | 시험에 출제되는 핵심 내용 요약 정리

048 빅데이터 단계별 프로세스★★★

[표 형태로 구성된 상세한 프로세스 단계별 내용]

단계 1: 분석 기획
- 비즈니스 이해 및 범위 설정
• 프로젝트를 착수하는 관계자(Stakeholders)의 이해를 정리시키기 위하여 구조적인 프로젝트 정의서의 SOW (Statement of Work)를 작성
• 프로젝트 목표 및 KPI(핵심성과지표), 목표수준의 정의와 같은 프로젝트 정량적 목표를 달성하기 위한 프로젝트 추진 조직, 역할 분담, 기간 등을 포함한 실행 계획의 추진조직 작성

단계 2: 데이터 준비
- 프로젝트 수행계획 수립
• 데이터 분석 목표 설계를 위한 설계 및 시스템 진행하며 빅데이터 분석 기능 기초를 마련
• 어려운 위험에 대한 대응 방안(Avoid), 전이(Transfer), 완화(Mitigate), 수용(Accept)으로 분류되어 위험 관리 계획 수립

단계 3: 데이터 분석
- 탐색적 분석
• 일반화적으로 관련 데이터베이스(RDBMS)를 사용하고 데이터 통합과 검증을 통하여 하여 데이터 스토어의 품질과 결과 구체화한다.
• 하둡, NoSQL 등을 이용하여 비정형 또는 반정형 데이터의 저장하기 위해 본격적 활용 방안을 설계합니다.

- 모델 설계 및 검증
• 그룹별 통의 데이터 수집을 위한 ETL 등의 다양한 구조 및 한량 데이터 수집을 위하여 더 데이터 수집과 저장법으로 개선합니다.

- 분석 결과 작성
• 분석 개발 다양한 분석 요구에 사전하고 데이터 즉시 대응 방법에 맞춤 수 있는 다양한 분석 도구 선정하고, 다원 데이터 선장한다.

분석용 데이터 준비:
• 가상 분석(Sentimental Analysis), 토픽 분석(Topic Analysis), 오피니언 마이닝(Opinion Analysis), 소셜 네트워크 분석(SNA) 등을 실시 테스트 분석을 네트워크로 분석 목적에 맞춘 삭정한 분석 기법을 구성한다.

단계 4: 데이터 분석
• 다양한 관점별을 기준 응용체를 선행하고 데이터 더는 분석의 분리와 기법적응을 이용한 측정법을 위한 기술을 수행할 수 있는 모 융합 간는라.

단계 5: 평가 및 모델링 인사
• 모델링인 분석 데이터를 이용한 기색 분상 등을 활용하여 중지 모범답 안전하기 대한 추구 모델링을 위한 기술을 수행한다.
1) 모델 분석
모향의 과정선이 방법률을 위하여 변색을 대

30 | 시험에 출제되는 핵심 내용 요약 정리

============================================================

페이지 31
============================================================

050 상향식 및 하향식 접근 방식의 혼용
: 디자인 씽킹★

• 디자인 씽킹(Design Thinking)은 상향식 접근 방식(Bottom-Up)과 하향식 접근 방식(Top-Down)을 혼합하여 활용하는 혁신적 문제 해결 방법론
• 사용자와 공감하고, 다양한 아이디어를 발산한 후 이를 수렴 및 검증하는 반복적이고 체계적인 방식으로, 다양한 도전 과제와 복잡한 문제를 효과적으로 해결

상향식 접근 방식(diverge) → 하향식 접근 방식(converse)

051 디자인 씽킹 프로세스 : 5단계★

• 디자인 씽킹(Design Thinking)은 사용자 중심으로 문제를 정의하고 해결방안을 찾아가는 반복적이고 유연한 프로세스이다.
• 이 과정은 공감 → 정의 → 아이디어 발산 → 프로토타이핑 → 테스트로 구성되며, 각 단계는 고객의 문제를 깊이 이해하고 효과적인 해결책을 도출하는 데 초점이 맞춰져 있다.

① Empathize(공감)
• 사용자나 이데이터와 관찰을 통해 고객의 문제를 이해하고 공감

② Define(문제정의)
• Empathize 단계에서 얻은 인사이트를 바탕으로 고객의 진짜 문제를 명확히 정의

③ Ideate(아이디어 발산)
• 문제정의 기반 기반으로 자유롭게 다양한 해결 방안을 제시

④ Prototype(프로토타입 제작)
• Ideate 단계에서 도출된 아이디어를 구체화하여 시제품(프로토타입)이나 서비스 시나리오 제작

⑤ Test(테스트)
• 프로토타입을 사용자에게 테스트하여 피드백 수집

제2과목 데이터의 분석 기획 | 31

============================================================

페이지 32
============================================================

052 하향식 접근 방식
(Top Down Approach)★★★

• 하향식 접근 방식은 명확히 주어진 문제나 전략적 목표를 기반으로 분석을 체계적으로 설계하고 실행하는 접근법이다.
• 이 방식은 해당 분석에서 시작해, 문제를 데이터 문제로 변환하여 구체적이고 실행 가능한 해결방안을 도출하는 데 초점을 맞춘다.
• 하향식 접근법(Top-Down Approach)은 문제가 명확히 정의된 상황에서 문제를 해결하는 데 적합하지만, 새로운 문제를 탐색하거나 발견하는 데는 못 가지 한계를 가지고 있다.

① 문제탐색(Problem Discovery)
• 현황 분석을 통해 최인된 문제나 기회를 선충적으로 탐색
• 문제의 원인과 연관성을 파악하여 데이터를 기반으로 해결 가능성을 검토
• 새로운 사실 및 솔 솔루션이 중점을 두는 것이 아니라, 문제를 해결책을 때 발생하는 기차에 집중

② 문제정의(Problem Definition)
• 문제 정의 단계는 비즈니스 문제를 데이터 분석 문제로 변환하여, 필요한 데이터와 분석 기법을 명확히 정의하는 단계
• 데이터 분석의 효율성을 높이고, 해결 과정을 체계적으로 설계하는 데 중요한 역할을 한다.

③ 해결방안 탐색(Solution Search)
• 해결방안 탐색 단계는 이전 단계에서 정의된 데이터 분석 문제를 해결하기 위해 다양한 방안을 탐색하고 비교하는 과정
• 분석의 효율성과 효과를 극대화하기 위해 사용 가능한 데이터, 도구, 예산, 기술적 요구 사항 등을 다각적으로 검토하는 데 중점을 둠

④ 타당성 검토(Feasibility Study)
• 타당성 검토 단계는 해결 방안 탐색에서 도출된 대안을 실제로 실행 가능한지 평가하는 과정이다.
• 이 단계는 경제적 타당도와 데이터 및 기술적 타당도의 두 가지 관점에서 분석하여, 프로젝트의 성공 가능성을 사전에 확인하고 리스크를 최소화한다.

32 | 시험에 출제되는 핵심 내용 요약 정리

============================================================

페이지 33
============================================================

053 문제 탐색(Problem Discovery) 단계
: 비즈니스 모델 기반 문제 탐색★

• 비즈니스 모델 기반 문제 탐색은 비즈니스 모델 캔버스를 활용하여 기업 내 가치 창출과 관련된 문제를 구조적으로 도출하는 단계이다.
• 이 과정은 업무, 제품, 고객, 규제와 감사, 지원 인프라 등 5가지 핵심 영역에서 문제를 발굴하여 데이터 분석 기회를 식별하는 데 초점이 맞춰져 있다.

32 | 시험에 출제되는 핵심 내용 요약 정리

============================================================

페이지 34
============================================================

059 빅데이터 환경에서 프로토타이핑의 역할★

① 문제 정의의 불명확성 해소
• 프로토타이핑을 통해 문제를 시각화하고, 데이터를 분석해 조기 검토를 제시함으로써 문제의 본질을 이해하고 구체화하는 데 도움을 제공

② 필요 데이터 준체 여부 확인
• 프로토타이핑은 필요한 데이터를 빠르게 탐색하고, 데이터 부족 또는 부정할 문제를 조기에 발견할 수 있도록 돕는다.

③ 데이터 사용 목적의 기법별 지원
• 프로토타이핑은 초기 목적이 업데이터 영고, 새로운 목적이나 목표로 전환할 수 있는 유연성을 제공

060 분석 과제 정의서★

• 분석 과제 정의서는 프로젝트의 목표, 데이터 요구사항, 분석 방법, 수행 주기 등을 체계적으로 정리한 문서
• 이를 통해 이해관계자 간의 합의를 강화하고, 분석 결과의 신뢰성과 활용성을 극대화할 수 있다.

061 분석 프로젝트 관리 방안★★★

• 분석 프로젝트는 도출된 분석 기회를 통해 구체적인 가치를 창출하고 목표를 달성하기 위한 체계적인 관리가 필요하다.
• 일반적인 프로젝트 관리 요소(범위, 일정, 품질, 리스크 등) 외에도, 분석 프로젝트는 데이터와 분석 모델의 특수성을 반영한 5가지 주요 속성을 중심으로 관리가 이루어져야 한다.

① Data Size(데이터 크기)
• 데이터 양이 방대할수록 처리 속도와 차원 관리가 중요

② Data Complexity(데이터 복잡성)
• 정형 데이터뿐 아니라 비정형·반정형 데이터(텍스트, 이미지 등)을 포함할 경우, 데이터 확보와 모델 선정이 더욱 까다로움

③ Speed(속도)
• 분석 결과를 도출하는 시간적 요구사항

④ Analytic Complexity(분석 복잡성)
• 분석 모델의 복잡도가 증가할수록 정확도(Accuracy)는 높아지지만, 모델 해석과 적용이 어려워질 복잡성과 정확도 간의 균형을 고려한 기준적 설정

⑤ Accuracy & Precision(정확도와 정밀도)
• Accuracy와 Precision 속성은 트레이드 오프(trade-off) 관계를 가지며, 분석 모델의 해석 및 적용 단계에서 사전 고려가 필요
• 정확도(Accuracy) : 모델이 예측 값이 실제 값과 얼마나 가까운지
• 정밀도(Precision) : 모델을 반복 적용했을 때 결과가 얼마나 일관되게 나오는지
• Accuracy와 Precision을 고려한 모델 설계에서 활용이 중요하다면 Accuracy를 우선, 안정성이 중요하다면 Precision을 우선

34 | 시험에 출제되는 핵심 내용 요약 정리

============================================================

페이지 35
============================================================

062 분석 프로젝트 영역별 주요 관리 항목★

• 데이터 분석 프로젝트는 다양한 관리 영역에서 세부적인 관리를 통해 성공적인 결과를 도출할 수 있다.

① 범위(Scope)
• 프로젝트의 목표와 범위를 명확히 정의

② 시간(Time)
• 타임박싱(Timeboxing) 기법을 활용하여 일정 관리를 유연하게 진행
• 품질을 보장하면서 일정 초과를 방지하는 관리 전략 적용

③ 원가(Cost)
• 프로젝트 수행에 필요한 데이터 처리 비용, 분석 도구, 인력 비용 등을 관리

④ 품질(Quality)
• 데이터 처리 및 분석 프로세스 준수 여부 확인

⑤ 통합(Integration)
• 다양한 데이터 소스와 분석 기법의 통합관리

⑥ 조달(Procurement)
• 조달은 프로젝트를 수행하는 데 필요한 자원(인프라, 데이터, 소프트웨어, 기술 등)을 확보하고, 이를 효과적으로 관리하여 프로젝트 관리 프로세스가 통합적으로 운영될 수 있도록 지원하는 중요한 활동
• PoC 프로젝트는 인프라를 구매하지 않고도 클라우드 서비스의 외부 소싱을 통해 신속하고 유연하게 진행할 수 있다.
• 이를 통해 초기 투자 비용을 절감하고, 프로젝트 성공 가능성을 높이는 동시에 내부 역량을 보완할 수 있다.

⑦ 자원(Resource)
• 프로젝트 수행에 필요한 인력, 기술, 시스템 등의 리소스를 관리

⑧ 리스크(Risk)
• 데이터 품질, 분석 오류 등 프로젝트와 관련된 위험을 식별하고 대응 방안을 수립

⑨ 의사소통(Communication)
• 이해관계자 및 팀 간 명확하고 원활한 의사소통 체계를 구축

⑩ 이해관계자(Stakeholder)
• 프로젝트에 영향을 미치는 모든 이해관계자를 식별하고 관리

063 분석 마스터 플랜★

• 분석 마스터 플랜은 조직이 데이터 분석을 통해 목표를 달성하기 위해 분석 과제 도출, 우선순위 평가, 세부 실행 계획 수립, 중·장기 로드맵 설계를 체계적으로 작성하는 계획

064 마스터 플랜 수립 프레임 워크★★

[THIS IS DIAGRAM: A flowchart showing the master plan framework with boxes for different stages including "전략적 중요도", "업무 내재화 적용 수준", "분석 과제 발굴 프로세스", and "분석 구현 로드맵 수립"]

065 ROI 관점에서 보는 빅데이터 4V★

• ROI 관점에서 빅데이터 4V는 Volume, Variety, Velocity가 투자 비용 요소로 작용하며, Value는 비즈니스 효과를 나타낸다.

[THIS IS DIAGRAM: A 3D cube diagram showing Volume, Variety, Velocity axes leading to Value with business return]

빅데이터 특성을 고려한 분석 ROI 요소

제2과목 데이터의 분석 기획 | 35

============================================================
